{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":783730,"sourceType":"datasetVersion","datasetId":407960},{"sourceId":11312647,"sourceType":"datasetVersion","datasetId":2912461}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\n\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, get_scheduler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:19.644098Z","iopub.execute_input":"2025-07-25T09:46:19.644363Z","iopub.status.idle":"2025-07-25T09:46:32.343313Z","shell.execute_reply.started":"2025-07-25T09:46:19.644341Z","shell.execute_reply":"2025-07-25T09:46:32.342763Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import json\n\n# Step 1: Load first sample from the .jsonl file\nwith open(\"/kaggle/input/the-natural-questions-dataset/simplified-nq-train.jsonl\", \"r\") as f:\n    first_line = f.readline()\n    sample = json.loads(first_line)\n\n# Step 2: Extract fields\nquestion = sample['question_text']\ncontext_tokens = sample['document_text'].split()  # space-tokenized\nannotations = sample['annotations'][0] if sample['annotations'] else None\n\n# Step 3: Extract short answer using start/end tokens\nif annotations and annotations['short_answers']:\n    start = annotations['short_answers'][0]['start_token']\n    end = annotations['short_answers'][0]['end_token']\n    answer_tokens = context_tokens[start:end]\n    answer = ' '.join(answer_tokens)\nelse:\n    answer = \"[NO SHORT ANSWER FOUND]\"\n\n# Step 4: Join full context (you may truncate later for max length)\ncontext = ' '.join(context_tokens)\n\n# Step 5: Construct prompt format\nprompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n\n# Step 6: Combine prompt + answer\nfull_input = f\"{prompt} {answer}\"\n\n# Step 7: Print final result\nprint(\"===== GPT Training Sample =====\")\nprint(full_input[:1000])  # Truncated for readability\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.344317Z","iopub.execute_input":"2025-07-25T09:46:32.344705Z","iopub.status.idle":"2025-07-25T09:46:32.369964Z","shell.execute_reply.started":"2025-07-25T09:46:32.344684Z","shell.execute_reply":"2025-07-25T09:46:32.369402Z"}},"outputs":[{"name":"stdout","text":"===== GPT Training Sample =====\nQuestion: which is the most common use of opt-in e-mail marketing\nContext: Email marketing - Wikipedia <H1> Email marketing </H1> Jump to : navigation , search <Table> <Tr> <Td> </Td> <Td> ( hide ) This article has multiple issues . Please help improve it or discuss these issues on the talk page . ( Learn how and when to remove these template messages ) <Table> <Tr> <Td> </Td> <Td> This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed . ( September 2014 ) ( Learn how and when to remove this template message ) </Td> </Tr> </Table> <Table> <Tr> <Td> </Td> <Td> This article possibly contains original research . Please improve it by verifying the claims made and adding inline citations . Statements consisting only of original research should be removed . ( January 2015 ) ( Learn how and when to remove this template message ) </Td> </Tr> </Table> ( Learn how and w\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.370632Z","iopub.execute_input":"2025-07-25T09:46:32.371367Z","iopub.status.idle":"2025-07-25T09:46:32.374668Z","shell.execute_reply.started":"2025-07-25T09:46:32.371343Z","shell.execute_reply":"2025-07-25T09:46:32.374095Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 1024, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": True       # Query-Key-Value bias\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.376267Z","iopub.execute_input":"2025-07-25T09:46:32.376479Z","iopub.status.idle":"2025-07-25T09:46:32.393620Z","shell.execute_reply.started":"2025-07-25T09:46:32.376464Z","shell.execute_reply":"2025-07-25T09:46:32.393069Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Normalization, GELU and Feed Forward","metadata":{}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n            GELU(), ## Activation\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.394212Z","iopub.execute_input":"2025-07-25T09:46:32.394450Z","iopub.status.idle":"2025-07-25T09:46:32.412697Z","shell.execute_reply.started":"2025-07-25T09:46:32.394427Z","shell.execute_reply":"2025-07-25T09:46:32.412111Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Multi-Head Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=True):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.413350Z","iopub.execute_input":"2025-07-25T09:46:32.414033Z","iopub.status.idle":"2025-07-25T09:46:32.437159Z","shell.execute_reply.started":"2025-07-25T09:46:32.414007Z","shell.execute_reply":"2025-07-25T09:46:32.436428Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Transformer Block","metadata":{}},{"cell_type":"code","source":"  class TransformerBlock(nn.Module):\n      def __init__(self, cfg):\n          super().__init__()\n          self.att = MultiHeadAttention(\n              d_in=cfg[\"emb_dim\"],\n              d_out=cfg[\"emb_dim\"],\n              context_length=cfg[\"context_length\"],\n              num_heads=cfg[\"n_heads\"],\n              dropout=cfg[\"drop_rate\"],\n              qkv_bias=cfg[\"qkv_bias\"])\n          self.ff = FeedForward(cfg)\n          self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n          self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n          self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n      def forward(self, x):\n          # Shortcut connection for attention block\n          shortcut = x\n          x = self.norm1(x)\n          x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n          x = self.drop_shortcut(x)\n          x = x + shortcut  # Add the original input back\n\n          # Shortcut connection for feed forward block\n          shortcut = x\n          x = self.norm2(x)\n          x = self.ff(x)\n          # 2*4*768\n          x = self.drop_shortcut(x)\n          x = x + shortcut  # Add the original input back\n\n          return x\n          # 2*4*768","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.437804Z","iopub.execute_input":"2025-07-25T09:46:32.437973Z","iopub.status.idle":"2025-07-25T09:46:32.459845Z","shell.execute_reply.started":"2025-07-25T09:46:32.437958Z","shell.execute_reply":"2025-07-25T09:46:32.459290Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## GPT Class","metadata":{}},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.460649Z","iopub.execute_input":"2025-07-25T09:46:32.460897Z","iopub.status.idle":"2025-07-25T09:46:32.481154Z","shell.execute_reply.started":"2025-07-25T09:46:32.460871Z","shell.execute_reply":"2025-07-25T09:46:32.480572Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model = GPTModel(GPT_CONFIG_124M)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:32.481798Z","iopub.execute_input":"2025-07-25T09:46:32.482021Z","iopub.status.idle":"2025-07-25T09:46:33.963146Z","shell.execute_reply.started":"2025-07-25T09:46:32.482005Z","shell.execute_reply":"2025-07-25T09:46:33.962552Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 163,037,184\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Loading Weights","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\ndef load_gpt2_from_kaggle(model_size, kaggle_dataset_path=\"/kaggle/input/openai-gpt2-weights/124M\"):\n    # Validate model size\n    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n    if model_size not in allowed_sizes:\n        raise ValueError(f\"Model size not in {allowed_sizes}\")\n\n    # Construct model directory path inside Kaggle dataset\n    model_dir = os.path.join(kaggle_dataset_path, model_size)\n\n    # Check if directory exists\n    if not os.path.isdir(model_dir):\n        raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n\n    # Load hyperparameters\n    hparams_path = os.path.join(model_dir, \"hparams.json\")\n    if not os.path.exists(hparams_path):\n        raise FileNotFoundError(f\"hparams.json not found in {model_dir}\")\n    settings = json.load(open(hparams_path))\n\n    # Load TensorFlow checkpoint\n    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n    if tf_ckpt_path is None:\n        raise FileNotFoundError(f\"No checkpoint found in {model_dir}\")\n    \n    # Extract parameters from checkpoint\n    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n\n    return settings, params\n\ndef load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n    # Initialize parameters dictionary with empty blocks for each layer\n    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n\n    # Iterate over each variable in the checkpoint\n    for name, _ in tf.train.list_variables(ckpt_path):\n        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n        variable_name_parts = name.split(\"/\")[1:]  # Skip 'model/' prefix\n\n        target_dict = params\n        if variable_name_parts[0].startswith(\"h\"):\n            layer_number = int(variable_name_parts[0][1:])\n            target_dict = params[\"blocks\"][layer_number]\n\n        for key in variable_name_parts[1:-1]:\n            target_dict = target_dict.setdefault(key, {})\n        \n        last_key = variable_name_parts[-1]\n        target_dict[last_key] = variable_array\n\n    return params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:33.965200Z","iopub.execute_input":"2025-07-25T09:46:33.965565Z","iopub.status.idle":"2025-07-25T09:46:51.441151Z","shell.execute_reply.started":"2025-07-25T09:46:33.965545Z","shell.execute_reply":"2025-07-25T09:46:51.440189Z"}},"outputs":[{"name":"stderr","text":"2025-07-25 09:46:36.823720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753436797.168342      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753436797.260983      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nsettings, params = load_gpt2_from_kaggle(model_size=\"124M\")\nprint(\"Settings:\", settings)\nprint(\"Parameter dictionary keys:\", params.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:51.442027Z","iopub.execute_input":"2025-07-25T09:46:51.442709Z","iopub.status.idle":"2025-07-25T09:46:55.266473Z","shell.execute_reply.started":"2025-07-25T09:46:51.442686Z","shell.execute_reply":"2025-07-25T09:46:55.265696Z"}},"outputs":[{"name":"stdout","text":"Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\nParameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import numpy as np\n\ndef assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n    return torch.nn.Parameter(torch.tensor(right))\n    \ndef load_weights_into_gpt(gpt, params):\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n\n    for b in range(len(params[\"blocks\"])):\n        q_w, k_w, v_w = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight = assign(\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight = assign(\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight = assign(\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias = assign(\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\n        gpt.trf_blocks[b].att.W_key.bias = assign(\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\n        gpt.trf_blocks[b].att.W_value.bias = assign(\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\n\n        gpt.trf_blocks[b].att.out_proj.weight = assign(\n            gpt.trf_blocks[b].att.out_proj.weight,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\n            gpt.trf_blocks[b].att.out_proj.bias,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].norm1.scale = assign(\n            gpt.trf_blocks[b].norm1.scale,\n            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n        gpt.trf_blocks[b].norm1.shift = assign(\n            gpt.trf_blocks[b].norm1.shift,\n            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n        gpt.trf_blocks[b].norm2.scale = assign(\n            gpt.trf_blocks[b].norm2.scale,\n            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n        gpt.trf_blocks[b].norm2.shift = assign(\n            gpt.trf_blocks[b].norm2.shift,\n            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:55.267342Z","iopub.execute_input":"2025-07-25T09:46:55.267625Z","iopub.status.idle":"2025-07-25T09:46:55.279248Z","shell.execute_reply.started":"2025-07-25T09:46:55.267597Z","shell.execute_reply":"2025-07-25T09:46:55.278584Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"load_weights_into_gpt(model,params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:55.279942Z","iopub.execute_input":"2025-07-25T09:46:55.280252Z","iopub.status.idle":"2025-07-25T09:46:55.652375Z","shell.execute_reply.started":"2025-07-25T09:46:55.280230Z","shell.execute_reply":"2025-07-25T09:46:55.651535Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Weights loaded finally hehe!!!!!!!","metadata":{}},{"cell_type":"markdown","source":"## Create Formatted Sample from QA Dataset","metadata":{}},{"cell_type":"code","source":"import json\nimport re\n\ndef clean_text(text):\n    \"\"\"Removes HTML tags and common entities from a text string.\"\"\"\n    text = re.sub(r\"<.*?>\", \"\", text)          # Remove HTML tags\n    text = re.sub(r\"&nbsp;|&amp;\", \" \", text)  # Replace common HTML entities\n    return text.strip()\n\ndef extract_context_with_answer(tokens, start, end, window=256):\n    \"\"\"Extract a window of context centered around the short answer span.\"\"\"\n    context_len = len(tokens)\n    mid = (start + end) // 2\n    half_window = window // 2\n    start_idx = max(0, mid - half_window)\n    end_idx = min(context_len, start_idx + window)\n    start_idx = max(0, end_idx - window)\n    return tokens[start_idx:end_idx]\n\nsamples = []\nN=600\nwith open(\"/kaggle/input/the-natural-questions-dataset/simplified-nq-train.jsonl\", \"r\") as f:\n    for i, line in enumerate(f):\n        if len(samples) >= N: break\n        example = json.loads(line)\n\n        # Skip examples without short answers\n        if not example['annotations'] or not example['annotations'][0]['short_answers']:\n            continue\n\n        # Raw, uncleaned tokens (needed for indexing)\n        raw_tokens = example['document_text'].split()\n        question = example['question_text']\n        sa = example['annotations'][0]['short_answers'][0]\n        start, end = sa['start_token'], sa['end_token']\n\n        # Extract answer from raw tokens (MUST be done before cleaning)\n        answer_tokens = raw_tokens[start:end]\n        answer = \" \".join(answer_tokens)\n\n        # Now clean both answer and context\n        cleaned_answer = clean_text(answer)\n        context_tokens = extract_context_with_answer(raw_tokens, start, end, window=256)\n        cleaned_context = clean_text(\" \".join(context_tokens))\n\n        samples.append({\n            \"question\": question,\n            \"context\": cleaned_context,\n            \"answer\": cleaned_answer\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:55.653325Z","iopub.execute_input":"2025-07-25T09:46:55.653626Z","iopub.status.idle":"2025-07-25T09:46:57.011563Z","shell.execute_reply.started":"2025-07-25T09:46:55.653604Z","shell.execute_reply":"2025-07-25T09:46:57.010973Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"samples[10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:57.012165Z","iopub.execute_input":"2025-07-25T09:46:57.012338Z","iopub.status.idle":"2025-07-25T09:46:57.017532Z","shell.execute_reply.started":"2025-07-25T09:46:57.012324Z","shell.execute_reply":"2025-07-25T09:46:57.016957Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'question': 'when do the eclipse supposed to take place',\n 'context': 'Solar eclipse of August 21 , 2017 - Wikipedia  Solar eclipse of August 21 , 2017  Jump to : navigation , search    Solar eclipse of August 21 , 2017     Totality as seen from Simpsonville , South Carolina     Map     Type of eclipse     Nature   Total     Gamma   0.4367     Magnitude   1.0306     Maximum eclipse     Duration   160 sec ( 2 m 40 s )       37 ° 00 ′ N 87 ° 42 ′ W \\ufeff / \\ufeff 37 ° N 87.7 ° W \\ufeff / 37 ; - 87.7     Max . width of band   115 km ( 71 mi )     Times ( UTC )     ( P1 ) Partial begin   15 : 46 : 48     ( U1 ) Total begin   16 : 48 : 32     Greatest eclipse   18 : 26 : 40     ( U4 ) Total end   20 : 01 : 35     ( P4 ) Partial end   21 : 04 : 19     References     Saros   145 ( 22 of 77 )',\n 'answer': 'August 21 , 2017'}"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"class GPTQADataset(Dataset):\n    def __init__(self, samples, tokenizer, max_length=512):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        prompt = f\"Question: {sample['question']}\\nContext: {sample['context']}\\nAnswer:\"\n        answer = sample[\"answer\"]\n        full = prompt + \" \" + answer\n\n        encodings = self.tokenizer(full,\n                                   max_length=self.max_length,\n                                   truncation=True,\n                                   padding=\"max_length\",\n                                   return_tensors=\"pt\")\n\n        labels = encodings.input_ids.clone()\n        prompt_len = len(self.tokenizer(prompt, truncation=True)[\"input_ids\"])\n        labels[0][:prompt_len] = -100  # Ignore loss on prompt\n        valid_label_count = (labels != -100).sum().item()\n        if valid_label_count == 0:\n            print(\"⚠️ WARNING: All labels are -100 (ignored in loss). Check prompt length.\")\n            print(f\"Prompt: {prompt}\")\n            print(f\"Answer: {answer}\")\n            print(f\"Label shape: {labels.shape}\")\n            print(f\"Labels: {labels}\")\n\n\n        return encodings.input_ids.squeeze(), labels.squeeze()\n\n\nfrom torch.utils.data import DataLoader\n\n\ndef create_qa_dataloader(samples, tokenizer, batch_size=4, shuffle=True):\n    dataset = GPTQADataset(samples, tokenizer)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:57.018138Z","iopub.execute_input":"2025-07-25T09:46:57.018475Z","iopub.status.idle":"2025-07-25T09:46:57.037112Z","shell.execute_reply.started":"2025-07-25T09:46:57.018460Z","shell.execute_reply":"2025-07-25T09:46:57.036634Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:57.037756Z","iopub.execute_input":"2025-07-25T09:46:57.038221Z","iopub.status.idle":"2025-07-25T09:46:58.700679Z","shell.execute_reply.started":"2025-07-25T09:46:57.038198Z","shell.execute_reply":"2025-07-25T09:46:58.699870Z"},"_kg_hide-input":false},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347ace7b498146f09449937cededfd0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7db50956acd4dd788d7e98a2dfcd82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647c45799d2c40c3927255285e564b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46c661b38e034f3b8a407d63e87f52ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34b48e847734f3ca081261382564a88"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_samples, val_samples = train_test_split(samples, test_size=0.1, random_state=42)\n\ntrain_loader = create_qa_dataloader(train_samples, tokenizer, batch_size=4)\nval_loader = create_qa_dataloader(val_samples, tokenizer, batch_size=4, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:58.701613Z","iopub.execute_input":"2025-07-25T09:46:58.701894Z","iopub.status.idle":"2025-07-25T09:46:58.706994Z","shell.execute_reply.started":"2025-07-25T09:46:58.701874Z","shell.execute_reply":"2025-07-25T09:46:58.706432Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Training and Evaluation","metadata":{}},{"cell_type":"code","source":"def calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:58.707718Z","iopub.execute_input":"2025-07-25T09:46:58.707995Z","iopub.status.idle":"2025-07-25T09:46:58.731332Z","shell.execute_reply.started":"2025-07-25T09:46:58.707971Z","shell.execute_reply":"2025-07-25T09:46:58.730828Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def generate_text_simple(model, idx, max_new_tokens, context_size):\n    # idx is (batch, n_tokens) array of indices in the current context\n\n    ###Input batch:\n ###tensor([[6109, 3626, 6100,  345],\n        ##[6109, 1110, 6622,  257]])\n\n    for _ in range(max_new_tokens):\n\n        # Crop current context if it exceeds the supported context size\n        # E.g., if LLM supports only 5 tokens, and the context size is 10\n        # then only the last 5 tokens are used as context\n        idx_cond = idx[:, -context_size:]\n\n        # Get the predictions\n        with torch.no_grad():\n            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n\n        # Focus only on the last time step\n        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n        logits = logits[:, -1, :]\n\n        # Apply softmax to get probabilities\n        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n\n        # Get the idx of the vocab entry with the highest probability value\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n\n        # Append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n\n    return idx\n\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:46:58.732167Z","iopub.execute_input":"2025-07-25T09:46:58.732805Z","iopub.status.idle":"2025-07-25T09:46:58.751498Z","shell.execute_reply.started":"2025-07-25T09:46:58.732780Z","shell.execute_reply":"2025-07-25T09:46:58.750770Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context, tokenizer):\n    # Initialize lists to track losses and tokens seen\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    # Main training loop\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward() # Calculate loss gradients\n            optimizer.step() # Update model weights using loss gradients\n            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n            global_step += 1\n\n            # Optional evaluation step\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n        # Print a sample text after each epoch\n        generate_and_print_sample(\n            model, tokenizer, device, start_context\n        )\n\n    return train_losses, val_losses, track_tokens_seen\n\n\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]  # max sequence length (block size)\n\n    # Encode the prompt\n    encoded = tokenizer.encode(start_context, return_tensors=\"pt\").to(device)\n\n    # Generate continuation\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model,\n            idx=encoded,\n            max_new_tokens=50,\n            context_size=context_size\n        )\n\n    # Decode and print\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(\"📌 Sample Output:\")\n    print(decoded_text.replace(\"\\n\", \" \"))  # compact view\n\n    model.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###### Note:\n# Uncomment the following code to calculate the execution time\nimport time\nstart_time = time.time()\n\ntorch.manual_seed(123)\n# model = GPTModel(GPT_CONFIG_124M)\n# model.to(device)›\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    device=device,\n    num_epochs=num_epochs,\n    eval_freq=10,        # evaluate every N steps\n    eval_iter=10,        # how many batches to use for quick val\n    start_context=\"Question: What is the capital of France?\\nContext: France is in Europe. Its capital is Paris.\\nAnswer:\",\n    tokenizer=tokenizer\n)\n\n# Note:\n# Uncomment the following code to show the execution time\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Ep 1 (Step 000090): Train loss 0.005, Val loss 0.000\nEp 1 (Step 000100): Train loss 0.002, Val loss 0.001\nEp 1 (Step 000110): Train loss 0.000, Val loss 0.001\nEp 1 (Step 000120): Train loss 0.002, Val loss 0.002\nEp 1 (Step 000130): Train loss 0.001, Val loss 0.001\n📌 Sample Output:\nQuestion: What is the capital of France? Context: France is in Europe. Its capital is Paris. Answer:::::::::::::::::::::::::::::::::::::::::::::::::::\nEp 2 (Step 000140): Train loss 0.009, Val loss 0.001\nEp 2 (Step 000150): Train loss 0.001, Val loss 0.000\nEp 2 (Step 000160): Train loss 0.000, Val loss 0.001\nEp 2 (Step 000170): Train loss 0.000, Val loss 0.000\nEp 2 (Step 000180): Train loss 0.027, Val loss 0.000\nEp 2 (Step 000190): Train loss 0.000, Val loss 0.000\nEp 2 (Step 000200): Train loss 0.000, Val loss 0.000\nEp 2 (Step 000210): Train loss 0.004, Val loss 0.000\nEp 2 (Step 000220): Train loss 0.000, Val loss 0.000\nEp 2 (Step 000230): Train loss 0.021, Val loss 0.000\nEp 2 (Step 000240): Train loss 0.001, Val loss 0.001\nEp 2 (Step 000250): Train loss 0.001, Val loss 0.000\nEp 2 (Step 000260): Train loss 0.002, Val loss 0.001\n📌 Sample Output:\nQuestion: What is the capital of France? Context: France is in Europe. Its capital is Paris. Answer::::::::::: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\nEp 3 (Step 000270): Train loss 0.005, Val loss 0.001\nEp 3 (Step 000280): Train loss 0.002, Val loss 0.001\nEp 3 (Step 000290): Train loss 0.001, Val loss 0.000\nEp 3 (Step 000300): Train loss 0.001, Val loss 0.001\nEp 3 (Step 000310): Train loss 0.000, Val loss 0.001\nEp 3 (Step 000320): Train loss 0.001, Val loss 0.001\nEp 3 (Step 000330): Train loss 0.000, Val loss 0.001\nEp 3 (Step 000340): Train loss 0.001, Val loss 0.001\nEp 3 (Step 000350): Train loss 0.001, Val loss 0.001\nEp 3 (Step 000360): Train loss 0.001, Val loss 0.001\nEp 3 (Step 000370): Train loss 0.001, Val loss 0.002\nEp 3 (Step 000380): Train loss 0.003, Val loss 0.001\nEp 3 (Step 000390): Train loss 0.002, Val loss 0.001\nEp 3 (Step 000400): Train loss 0.003, Val loss 0.001\n📌 Sample Output:\nQuestion: What is the capital of France? Context: France is in Europe. Its capital is Paris. Answer:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\nEp 4 (Step 000410): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000420): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000430): Train loss 0.014, Val loss 0.001\nEp 4 (Step 000440): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000450): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000460): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000470): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000480): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000490): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000500): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000510): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000520): Train loss 0.000, Val loss 0.001\nEp 4 (Step 000530): Train loss 0.000, Val loss 0.001\n📌 Sample Output:\nQuestion: What is the capital of France? Context: France is in Europe. Its capital is Paris. Answer:::::::::::::::::::::::::::::::::::::::::::::::::::\nEp 5 (Step 000540): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000550): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000560): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000570): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000580): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000590): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000600): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000610): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000620): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000630): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000640): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000650): Train loss 0.000, Val loss 0.001\nEp 5 (Step 000660): Train loss 0.000, Val loss 0.002\nEp 5 (Step 000670): Train loss 0.001, Val loss 0.001\n📌 Sample Output:\nQuestion: What is the capital of France? Context: France is in Europe. Its capital is Paris. Answer:::::::::::::::::::::::::::::::::::::::::::::::::::\nEp 6 (Step 000680): Train loss 0.001, Val loss 0.001\nEp 6 (Step 000690): Train loss 0.002, Val loss 0.001\nEp 6 (Step 000700): Train loss 0.000, Val loss 0.001\nEp 6 (Step 000710): Train loss 0.000, Val loss 0.001\nEp 6 (Step 000720): Train loss 0.001, Val loss 0.001\nEp 6 (Step 000730): Train loss 0.000, Val loss 0.001\nEp 6 (Step 000740): Train loss 0.000, Val loss 0.001\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    # Plot training and validation loss against epochs\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n\n    # Create a second x-axis for tokens seen\n    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n    ax2.set_xlabel(\"Tokens seen\")\n\n    fig.tight_layout()  # Adjust layout to make room\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n    start_context=\"Question: Who is the founder of Microsoft?\\nContext: Bill Gates founded Microsoft.\\nAnswer:\", \n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}